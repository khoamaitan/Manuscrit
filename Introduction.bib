@article{LINGEMANN2005275,
title = "High-speed laser localization for mobile robots",
journal = "Robotics and Autonomous Systems",
volume = "51",
number = "4",
pages = "275 - 296",
year = "2005",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2005.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S0921889005000254",
author = "Kai Lingemann and Andreas Nüchter and Joachim Hertzberg and Hartmut Surmann",
keywords = "Localization, Pose tracking, Autonomous mobile robots, Scan matching, High-speed robotics"
}

@INPROCEEDINGS{525835,
	author={P. Weckesser and R. Dillmann and M. Elbs and S. Hampel},
	booktitle={Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots},
	title={Multiple sensor processing for high-precision navigation and environmental modeling with a mobile robot},
	year={1995},
	volume={1},
	number={},
	pages={453-458 vol.1},
	keywords={active vision;distance measurement;mobile robots;navigation;path planning;position measurement;sensor fusion;stereo image processing;active stereo vision;binocular vision;collision avoidance;environmental modeling;high-precision navigation;mobile robot;model-based image processing;multiple sensor processing;natural landmarks;obstacle detection;odometry;real-time position correction;simply structured environments;stereo images;structured light sensing;trinocular vision;ultrasonic sensing;Cameras;Mobile robots;Navigation;Optical sensors;Orbital robotics;Real time systems;Robot sensing systems;Robot vision systems;Sensor phenomena and characterization;Sensor systems},
	doi={10.1109/IROS.1995.525835},
	ISSN={},
	month={Aug},}

@inproceedings{forssen_maximally_2007,
	title = {Maximally {Stable} {Colour} {Regions} for {Recognition} and {Matching}},
	isbn = {978-1-4244-1179-5 978-1-4244-1180-1},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270145},
	doi = {10.1109/CVPR.2007.383120},
	urldate = {2016-06-13},
	publisher = {IEEE},
	author = {Forssen, Per-Erik},
	month = jun,
	year = {2007},
	pages = {1--8}
}

@INPROCEEDINGS{6957799,
	author={N. Bernini and M. Bertozzi and L. Castangia and M. Patander and M. Sabbatelli},
	booktitle={17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
	title={Real-time obstacle detection using stereo vision for autonomous ground vehicles: A survey},
	year={2014},
	volume={},
	number={},
	pages={873-878},
	keywords={object detection;road vehicles;stereo image processing;traffic engineering computing;2D sensor technologies;3D sensor technologies;autonomous ground vehicles;intelligent ground vehicle;real-time obstacle detection;stereo vision;Equations;Estimation;Mathematical model;Real-time systems;Roads;Stereo vision;Three-dimensional displays},
	doi={10.1109/ITSC.2014.6957799},
	ISSN={2153-0009},
	month={Oct},}

@conference{Guney2016ACCV,
	title = {Deep Discrete Flow},
	author = {G{\"u}ney, Fatma and Geiger, Andreas},
	booktitle = {Asian Conference on Computer Vision (ACCV)},
	year = {2016}
}

@inproceedings{azuma_egomotion_2010,
	title = {Egomotion estimation using planar and non-planar constraints},
	doi = {10.1109/IVS.2010.5548117},
	abstract = {There are two major approaches for estimating camera motion (egomotion) given an image sequence. Each approach has own strengths and weaknesses. One approach is the feature based methods. In this approach the point feature correspondences are taken as the input. Since initially the depths of point features are unknown, the egomotion is estimated by the depth independent epipolar constraints on the point feature correspondences. This approach is robust in practice, but is relatively limited in accuracy since it exploits no structure assumption, such as planarity. The other approach, termed the direct method, has the advantage in its accuracy. In this method, the egomotion is estimated as the parameters of a homography by directly aligning the planar potion of two images. The direct method may be preferable in the cases with known planes that are persistent in the view. The on-board camera system for ground vehicles is a representative example. Despite the potential accuracy, the direct method fails when the plane lacks proper texture. We propose an egomotion estimation method that is based on both the homographic constraint on a planar region, and on the epipolar constraint on generally non-planar regions, so that the both kinds of visual cues contribute to the estimation. We observe that the method improves the egomotion estimation in robustness while retaining the comparable accuracy to the direct method.},
	booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Azuma, T. and Sugimoto, S. and Okutomi, M.},
	month = jun,
	year = {2010},
	keywords = {camera motion estimation, depth independent epipolar constraints, egomotion estimation, ground vehicles, homographic constraint, image sensors, image sequences, Intelligent vehicles, Land vehicles, Layout, motion estimation, nonplanar constraints, on-board camera system, parameter estimation, planar constraints, Robustness, Simultaneous localization and mapping, Smart cameras, traffic engineering computing, USA Councils},
	pages = {855--862},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\MAI\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\8h4pxrbr.default\\zotero\\storage\\6JQW8RTV\\5548117.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\MAI\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\8h4pxrbr.default\\zotero\\storage\\ANRWGKEW\\Azuma et al. - 2010 - Egomotion estimation using planar and non-planar c.pdf:application/pdf}
}

@inproceedings{zhou2017unsupervised,
	Author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
	Title = {Unsupervised Learning of Depth and Ego-Motion from Video},
	Booktitle = {CVPR},
	Year = {2017}
}



@inproceedings{weiss_smoothness_1997,
	title = {Smoothness in layers: {Motion} segmentation using nonparametric mixture estimation},
	shorttitle = {Smoothness in layers},
	doi = {10.1109/CVPR.1997.609375},
	abstract = {Grouping based on common motion, or “common fate” provides a powerful cue for segmenting image sequences. Recently a number of algorithms have been developed that successfully perform motion segmentation by assuming that the motion of each group can be described by a low dimensional parametric model (e.g. affine). Typically the assumption is that motion segments correspond to planar patches in 3D undergoing rigid motion. Here we develop an alternative approach, where the motion of each group is described by a smooth dense flow field and the stability of the estimation is ensured by means of a prior distribution on the class of flow fields. We present a variant of the EM algorithm that can segment image sequences by fitting multiple smooth flow fields to the spatiotemporal data. Using the method of Green's functions, we show how the estimation of a single smooth flow field can be performed in closed form, thus making the multiple model estimation computationally feasible. Furthermore, the number of models is estimated automatically using similar methods to those used in the parametric approach. We illustrate the algorithm's performance on synthetic and real image sequences},
	booktitle = {, 1997 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 1997. {Proceedings}},
	author = {Weiss, Y.},
	month = jun,
	year = {1997},
	keywords = {common fate, common motion, Computational modeling, Computer vision, Green's function methods, Green's functions, image segmentation, image sequences, motion estimation, motion segmentation, multiple model estimation, multiple smooth flow fields, nonparametric mixture estimation, Parametric statistics, real image sequences, rigid motion, spatiotemporal data, Spatiotemporal phenomena, Stability},
	pages = {520--526},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\MAI\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\8h4pxrbr.default\\zotero\\storage\\RER4DPDB\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\MAI\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\8h4pxrbr.default\\zotero\\storage\\6HH89ZRK\\Weiss - 1997 - Smoothness in layers Motion segmentation using no.pdf:application/pdf}
}

@Article{DeCarlo2000,
	author="DeCarlo, Douglas
	and Metaxas, Dimitris",
	title="Optical Flow Constraints on Deformable Models with Applications to Face Tracking",
	journal="International Journal of Computer Vision",
	year="2000",
	month="Jul",
	day="01",
	volume="38",
	number="2",
	pages="99--127",
	abstract="Optical flow provides a constraint on the motion of a deformable model. We derive and solve a dynamic system incorporating flow as a hard constraint, producing a model-based least-squares optical flow solution. Our solution also ensures the constraint remains satisfied when combined with edge information, which helps combat tracking error accumulation. Constraint enforcement can be relaxed using a Kalman filter, which permits controlled constraint violations based on the noise present in the optical flow information, and enables optical flow and edge information to be combined more robustly and efficiently. We apply this framework to the estimation of face shape and motion using a 3D deformable face model. This model uses a small number of parameters to describe a rich variety of face shapes and facial expressions. We present experiments in extracting the shape and motion of a face from image sequences which validate the accuracy of the method. They also demonstrate that our treatment of optical flow as a hard constraint, as well as our use of a Kalman filter to reconcile these constraints with the uncertainty in the optical flow, are vital for improving the performance of our system.",
	issn="1573-1405",
	doi="10.1023/A:1008122917811",
	url="https://doi.org/10.1023/A:1008122917811"
}

@INPROCEEDINGS{6906584,
	author={C. Forster and M. Pizzoli and D. Scaramuzza},
	booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},
	title={SVO: Fast semi-direct monocular visual odometry},
	year={2014},
	volume={},
	number={},
	pages={15-22},
	keywords={autonomous aerial vehicles;control engineering computing;distance measurement;embedded systems;motion estimation;probability;robot vision;stereo image processing;3D points;GPS-denied environments;SVO;consumer laptop;fast semidirect monocular visual odometry;high frame-rate motion estimation;micro-aerial-vehicle state-estimation;onboard embedded computer;open-source software;outlier measurements;pixel intensities;probabilistic mapping method;subpixel precision;Cameras;Feature extraction;Motion estimation;Optimization;Robustness;Three-dimensional displays;Tracking},
	doi={10.1109/ICRA.2014.6906584},
	ISSN={1050-4729},
	month={May},}

@article{bouchafa_c-velocity:_2012,
	title = {c-{Velocity}: {A} {Flow}-{Cumulating} {Uncalibrated} {Approach} for 3D {Plane} {Detection}},
	volume = {97},
	issn = {0920-5691, 1573-1405},
	shorttitle = {c-{Velocity}},
	url = {http://link.springer.com/article/10.1007/s11263-011-0475-6},
	doi = {10.1007/s11263-011-0475-6},
	abstract = {This paper deals with plane detection from a monocular image sequence without camera calibration or a priori knowledge about the egomotion. Within a framework of driver assistance applications, it is assumed that the 3D scene is a set of 3D planes. In this paper, the vision process considers obstacles, roads and buildings as planar structures. These planes are detected by exploiting iso-velocity curves after optical flow estimation. A Hough Transform-like frame called c-velocity was designed. This paper explains how this c-velocity, defined by analogy to the v-disparity in stereovision, can represent planes, regardless of their orientation and how this representation facilitates plane extraction. Under a translational camera motion, planar surfaces are transformed into specific parabolas of the c-velocity space. The error and robustness analysis of the proposed technique confirms that this cumulative approach is very efficient for making the detection more robust and coping with optical flow imprecision. Moreover, the results suggest that the concept could be generalized to the detection of other parameterized surfaces than planes.},
	language = {en},
	number = {2},
	urldate = {2017-02-17},
	journal = {International Journal of Computer Vision},
	author = {Bouchafa, Samia and Zavidovique, Bertrand},
	month = apr,
	year = {2012},
	pages = {148--166},
	file = {Snapshot:C\:\\Users\\MAI\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\8h4pxrbr.default\\zotero\\storage\\RNBI8AMS\\s11263-011-0475-6.html:text/html}
}
